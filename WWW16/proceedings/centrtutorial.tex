\documentclass[pdfpagelabels=false]{sig-alternate-2013} % option is to shut down hyperref warnings
\setlength{\paperheight}{11in} % To shut down hyperref warnings
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{flushend}
\usepackage[numbers,square,sort&compress]{natbib}
\renewcommand{\refname}{References}
\renewcommand{\bibsection}{\section{References}}
\renewcommand{\bibfont}{\raggedright}

\permission{Copyright is held by the author/owner(s).}
\conferenceinfo{WWW'16 Companion,}{April 11--15, 2016, Montr\'eal, Qu\'ebec, Canada.}
\copyrightetc{ACM \the\acmcopyr}
\crdata{978-1-4503-4144-8/16/04. \\
http://dx.doi.org/10.1145/2872518.2891063}

\clubpenalty=10000
\widowpenalty = 10000

\begin{document}
\title{Centrality Measures on Big Graphs:\\Exact, Approximated, and Distributed Algorithms}

\numberofauthors{3}
\author{
\alignauthor
Francesco Bonchi\\
       \affaddr{ISI Foundation}\\
       \affaddr{Turin, Italy}\\
       \email{francescobonchi@acm.org}
\end{tabular}\begin{tabular}[t]{p{1.3\auwidth}}\centering
Gianmarco~De~Francisci~Morales\\
       \affaddr{Qatar Computing Research Institute}\\
       \affaddr{Doha, Qatar}\\
       \email{gdfm@acm.org}
\alignauthor
Matteo Riondato\titlenote{Main contact.}\\
       \affaddr{Two Sigma Investments}\\
       \affaddr{New York, NY, USA}\\
       \email{matteo@twosigma.com}
}

\date{12 February 2016}
\maketitle

\begin{abstract}
Centrality measures allow to measure the relative importance of a node or an
edge in a graph w.r.t.~other nodes or edges. Many measures of centrality have
been developed in the literature to capture different aspects of the informal
concept of importance, and algorithms for different measures have been proposed.
In this tutorial, we survey the different definitions of centrality measures and
the algorithms to compute them. We start from the most common measures (e.g.,
closeness centrality, betweenness centrality) and move to more complex ones,
glike spanning-edge centrality. In our presentation, we begin
from exact algorithms and then progress to approximation algorithms, including
sampling-based ones, and to highly-scalable MapReduce algorithms for huge
graphs, both for exact computation and for keeping the measures up-to-date on
dynamic graphs where edges are inserted or deleted over time. Our goal is to
show how advanced algorithmic techniques and scalable systems can be used to
obtain efficient algorithms for an important graph mining tasks, and to
encourage research in the area by highlighting open problems and possible
directions.
\end{abstract}

\category{G.2.2}{Discrete Mathematics}{Graph Theory}[Graph algorithms]
\category{H.2.8}{Database Management}{Database Applications}[Data mining]

\keywords{Centrality, Betweenness, Closeness}

\section{Introduction}
Identifying the ``important'' nodes or edges in a graph is a fundamental task
in network analysis, with many applications. Many measures, known as
\emph{centrality indices}, have been proposed over the years, formalizing the
concept of importance in different ways~\citep{Newman10}. Centrality measures
rely on graph properties to quantify importance. For example,betweenness
centrality, one of the most commonly used centrality indices, counts the number
of shortest paths going through a node, while the closeness centrality of a node
is the average sum of the inverse of the distance to other nodes. Other
centrality measures use eigenvectors, random walks, degrees, or more complex
properties. The PageRank index of a node is also a centrality measure, and
centrality measures for sets of nodes are also possible.

With the proliferation of huge networks with millions of nodes and billions of
edges, the importance of having scalable algorithms for computing centrality
indices has become more and more evident and a number of contributions have been
recently proposed, ranging from heuristics that perform extremely well in
practice to approximation algorithms offering strong probabilistic guarantees,
to scalable algorithms for the MapReduce platform. Moreover, the dynamic nature
of many networks, i.e., the addition and removal of nodes and/or edges over
time, dictates the need to keep the computed values of centrality up-to-date as
the graph changes. These challenging problems have enjoyed enormous interest
from the research community, with many relevant contributions proposed recently
to tackle them.

Our tutorial presents, in a unified framework, some of the many measures of
centrality and discuss the algorithms to compute them, both in an exact and in
an approximate way, both in-memory and in a parallel/distributed fashion for the
MapReduce framework of computation. This is done with an effort to ease the
comparison between different measures of centrality, the different quality
guarantees offered by approximation algorithms, and the different trade-offs and
scalability behaviors characterizing parallel/distributed algorithms. We believe
this unity of presentation is beneficial both for newcomers and for
experienced researchers in the field, who will be exposes to the material from a
unified point of view.

The graph analyst can now choose among a huge number of centrality indices, from
the well-established ones originally developed in sociology, to the ones more
recently introduced to capture other aspects of ``importance''.  At the same
time, the original algorithms that could handle the relatively small networks
for classic social science experiments have been superseded by important
algorithmic contributions that exploit modern computational frameworks and/or
obtain fast, high-quality approximations. It is our belief that the long history of
centrality measures and the ever-increasing interest from computer
scientists in analyzing larger and richer graphs, create the need for an
all-around organization of both old and new materials, and is the desire to
satisfy this need that inspired us to develop this tutorial.

\section{Outline}
The tutorial is structured in three main technical parts, plus a concluding part
where we discuss future research directions. All the three technical parts will
contains both theory and experimental results.

\paragraph{Part I: Definitions and Exact Algorithms}
In this first part, we introduce the different centrality measures, starting
from important axioms that a good centrality measure should require. We then
discuss the relationship between the different measures, including
results highlighting the high correlation between many of them. After having
laid these foundations, we move to present the algorithms for the exact
computation of centrality measures, both in static and in dynamic graph. We
discuss the state-of-the-art by presenting both algorithms with the best
worst-case time complexity and heuristics that work extremely well in practice
by exploiting different properties of real world graphs.

Exact computation of centrality measures becomes impractical on web-scale
networks. Commonly, one of two alternative approaches is taken to speed up the
computation: focus on obtaining an \emph{approximation} of the measure of
interest or using \emph{parallel and/or distributed algorithms}. In the second
part of our tutorial we explore the former approach, while in the third part we
deal with the latter.

\paragraph{Part II: Approximation Algorithms}
Most approximation algorithms for centrality measures use various forms of
sampling and more or less sophisticated analysis to derive a sample size
sufficient to achieve the desired level of approximation with the desired level
of confidence. In this part we present a number of these sampling based
algorithms, from simple ones using the Hoeffding inequality to more complex ones
using VC-dimension and Rademacher Averages. For each algorithm, we discuss its
merits and drawbacks, and highlight the challenges for the algorithm designer.
We also discuss the case of maintaining an approximation up-to-date in a dynamic
graphs, presenting a number of contributions that recently appeared in the
literature.

\paragraph{Part III: Highly-scalable Algorithms}
In the third part of our tutorial, we discuss parallel and distributed
algorithms for the computation of centrality measures in static and dynamic
graphs. Specifically we present an approach based on GPUs and one based on
processing parallel/distributed data streams, together with experimental
results.

\paragraph{List of topics with references}
The following is a preliminary list of topics we will cover in each part of the
tutorial, with the respective references.
\begin{enumerate}
	\item {\bf Introduction: definitions and exact algorithms}
		\begin{enumerate}
			\item The axioms of centrality~\citep{BoldiV14}
			\item Definitions of centrality~\citep{Newman10}, including, but not
				limited to: betweenness, closeness, degree, eigenvector,
				harmonic, Katz, absorbing random-walk~\citep{MavroforakisMG15},
				and spanning-edge centrality~\citep{MavroforakisGLKT15}.
			\item Betweenness centrality: exact algorithm~\citep{Brandes01} and
				heuristically-faster exact algorithms for betweenness
				centrality~\citep{ErdosIBT15,SariyuceSKC13}.
			\item Exact algorithms for betweenness centrality in a dynamic
				graph~\citep{LeeLPCC12,NasrePR14,PontecorviR15}.
			\item Exact algorithms for closeness centrality in a dynamic
				graph~\citep{SariyuceKSC13b}.
		\end{enumerate}
	\item {\bf Approximation algorithms}
		\begin{enumerate}
			\item Sampling-based algorithm for closeness
				centrality~\citep{EppsteinW04}.
			\item Betweenness centrality: almost-linear-time approximation
				algorithm~\citep{Yoshida14}, basic sampling-based
				algorithm~\citep{BrandesP07}, refined
				estimators~\citep{GeisbergerSS08}, VC-dimension bounds for
				betweenness centrality~\citep{RiondatoK15}.
			\item Approximation algorithms for betweenness centrality in dynamic
				graphs~\citep{KasWCC13,BergaminiMS14,BergaminiM15,HayashiAY15}.
		\end{enumerate}
	\item {\bf Highly-scalable algorithms}
		\begin{enumerate}
			\item GPU-based algorithms~\citep{SariyuceKSC13}.
			\item Exact parallel streaming algorithm for betweenness centrality in a
				dynamic graph~\citep{KourtellisMB15}.
		\end{enumerate}
	\item {\bf Challenges and directions for future research}
\end{enumerate}

\section{Intended Audience}
The tutorial is aimed at researchers interested in the theory and the
applications of algorithms for graph mining and social network analysis.

We do not require any specific existing knowledge. The tutorial is designed for
an audience of computer scientists who have a general idea of the problems and
challenges in graph analysis. We will present the material in such a way that
any advanced undergraduate student would be able to productively follow our
tutorial and we will actively engage with the audience and adapt our pace and
style to ensure that every attendee can benefit from our tutorial.
The tutorial starts from the basic definitions and progressively moves to more advanced
algorithms, including sampling-based approximation algorithms and MapReduce
algorithms, so that it will be of interest both to researchers new
to the field and to a more experienced audience.

%\section{Duration: \textrm{Half-day}}

%\section{Previous editions of the tutorial}
%The tutorial was not previously offered. We did not find any tutorial covering
%similar topics in the programs of recent relevant conferences.

\section{Support materials}
We developed a mini-website for the tutorial at
\url{http://matteo.rionda.to/centrtutorial/}. It contains the
abstract of the tutorial, a detailed outline with short a description of
each item of the outline, a full list of references with links to
electronic editions, a list of software packages implementing the
algorithms, and the slides used in the tutorial presentation.

\section{Instructors}
This tutorial is developed by Francesco Bonchi, Gianmarco De Francisci Morales,
and Matteo Riondato. All three instructors will attend the conference.

\smallskip
\noindent{\bf Francesco Bonchi} is Research Leader at the ISI Foundation, Turin, Italy,
where he leads the "Algorithmic Data Analytics" group. He is also Scientific
Director for Data Mining at Eurecat (Technological Center of Catalunya),
Barcelona. Before he was Director of Research at Yahoo Labs in Barcelona, Spain,
leading the Web Mining Research group.

His recent research interests include mining query-logs, social networks, and
social media, as well as the privacy issues related to mining these kinds of
sensible data.
%In the past he has been interested in data mining query
%languages, constrained pattern mining, mining spatiotemporal and mobility data,
%and privacy preserving data mining.

He will be PC Chair of the 16th IEEE International Conference on Data Mining
(ICDM 2016) to be held in Barcelona in December 2016. He is member of the ECML
PKDD Steering Committee, Associate Editor of the newly created IEEE Transactions
on Big Data (TBD), of the IEEE Transactions on Knowledge and Data Engineering
(TKDE), the ACM Transactions on Intelligent Systems and Technology (TIST),
Knowledge and Information Systems (KAIS), and member of the Editorial Board of
Data Mining and Knowledge Discovery (DMKD).  %
%He has been program co-chair of the
%European Conference on Machine Learning and Principles and Practice of Knowledge
%Discovery in Databases (ECML PKDD 2010). Dr. Bonchi has also served as program
%co-chair of the first and second ACM SIGKDD International Workshop on Privacy,
%Security, and Trust in KDD (PinKDD 2007 and 2008), the 1st IEEE International
%Workshop on Privacy Aspects of Data Mining (PADM 2006), and the 4th
%International Workshop on Knowledge Discovery in Inductive Databases (KDID
%2005).
He is co-editor of the book "Privacy-Aware Knowledge Discovery: Novel
Applications and New Techniques" published by Chapman \& Hall/CRC Press.
%He earned his Ph.D. in computer science from the University of Pisa in December 2003.

He presented a tutorial at ACM KDD'14.

\smallskip
\noindent{\bf Gianmarco De Francisci Morales} is a Visiting Scientist at Aalto
University. Previously he worked as a Research Scientist at Yahoo Labs
Barcelona, and as a Research Associate at ISTI-CNR in Pisa. His research focuses
on scalable data mining, with an emphasis on Web mining and data-intensive
scalable computing systems.
%He is an active member of the open source community of the Apache Software Foundation, working on the Hadoop ecosystem, and a committer for the Apache Pig project.
He is one of the lead developers of Apache SAMOA, an open-source platform for
mining big data streams. He presented a tutorial on stream mining at IEEE
BigData'14.

\smallskip
\noindent{\bf Matteo Riondato} is a Research Scientist in the Labs group at Two Sigma
Investments. Previously he was a postdoc at Stanford and at Brown. His
dissertation on sampling-based randomized algorithms for data and graph mining
received the Best Student Poster Award at SIAM SDM'14. His research focuses on
exploiting advanced theory in practical algorithms for time series
analysis, pattern mining, and social network analysis. He presented
tutorials at ACM KDD'15, ECML PKDD'15, and ACM CIKM'15.
\bibliographystyle{abbrvnat}
\bibliography{centrality}

\end{document}
